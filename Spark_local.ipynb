{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3982b234",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc455c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\n",
    "    f'org.apache.hadoop:hadoop-aws:3.3.1',\n",
    "    'com.google.guava:guava:30.1.1-jre',\n",
    "    'org.apache.httpcomponents:httpcore:4.4.14', \n",
    "    'com.google.inject:guice:4.2.2', \n",
    "    'com.google.inject.extensions:guice-servlet:4.2.2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c969ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:00:24 WARN Utils: Your hostname, DESKTOP-HT1RH4E resolves to a loopback address: 127.0.1.1; using 172.26.69.25 instead (on interface eth0)\n",
      "23/09/13 11:00:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/greg/.ivy2/cache\n",
      "The jars for the packages stored in: /home/greg/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.httpcomponents#httpcore added as a dependency\n",
      "com.google.inject#guice added as a dependency\n",
      "com.google.inject.extensions#guice-servlet added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-896e62c7-8136-45d6-998b-38d1e44ba5b3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.guava#guava;30.1.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.8.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound com.google.inject#guice;4.2.2 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound com.google.inject.extensions#guice-servlet;4.2.2 in central\n",
      ":: resolution report :: resolve 410ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.inject#guice;4.2.2 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-servlet;4.2.2 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.8.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.guava#guava;25.1-android by [com.google.guava#guava;30.1.1-jre] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-896e62c7-8136-45d6-998b-38d1e44ba5b3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/12ms)\n",
      "23/09/13 11:00:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\\\n",
    "    .setMaster(\"local\")\\\n",
    "    .setAppName(\"pyspark-unittests\")\\\n",
    "    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\\\n",
    "    .set('spark.jars.packages', ','.join(packages))\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2604011",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed6f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e5bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_json(\"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2474751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c95231",
   "metadata": {},
   "source": [
    "# Process data for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1ca9",
   "metadata": {},
   "source": [
    "Filter out all not logged-in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30be67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f1d79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f9a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fca17",
   "metadata": {},
   "source": [
    "## Get average session length and average number of items per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a6bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_session_len_per_user = spark.sql('''\n",
    "    SELECT userId, \n",
    "        AVG(sessionLength) AS avg_session_length, \n",
    "        AVG(items) AS avg_items_per_session \n",
    "    FROM (\n",
    "        SELECT userId,\n",
    "            MAX(ts) - MIN(ts) AS sessionLength, \n",
    "            MAX(itemInSession) AS items FROM events\n",
    "        GROUP BY userId, sessionId\n",
    "    )\n",
    "    GROUP BY userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c457111",
   "metadata": {},
   "source": [
    "## Get average time between sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e49271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import last, udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdc27e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:01:31 WARN TaskSetManager: Stage 0 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:34 WARN TaskSetManager: Stage 1 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------+\n",
      "|userId|sessionId|          end|\n",
      "+------+---------+-------------+\n",
      "|300011|       60|1538587993000|\n",
      "+------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session_ends = df.sort(\"ts\").groupby(df.userId, df.sessionId).agg(last(df.ts).alias(\"end\"))\n",
    "session_ends.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(FloatType())\n",
    "def time_bw_sessions(s: pd.Series) -> float:\n",
    "    return s.diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441b6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_bw_sessions = session_ends.sort(\"sessionId\") \\\n",
    "    .groupby(\"userId\").agg(time_bw_sessions(\"end\").alias(\"avg_time_bw_sessions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231cf76",
   "metadata": {},
   "source": [
    "## How much time per day is user spending on the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_day = spark.sql('''\n",
    "    SELECT events.userId, first(a.days), ROUND(COUNT(*) / first(a.days), 2) AS pages_per_day\n",
    "    FROM events\n",
    "    JOIN (\n",
    "        SELECT userId, GREATEST(1, ROUND((MAX(ts) - MIN(ts)) / 3600 / 24 / 1000, 2)) AS days FROM events\n",
    "        GROUP BY userId\n",
    "    ) AS a ON events.userId = a.userId\n",
    "    GROUP BY events.userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9ab19",
   "metadata": {},
   "source": [
    "## Find how many times a user visits each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea26897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_per_user_by_type = spark.sql('''\n",
    "    SELECT userId, page, COUNT(*) AS page_visits\n",
    "    FROM events\n",
    "    GROUP BY userId, page\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f395980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:01:37 WARN TaskSetManager: Stage 7 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pages_per_user_by_type = pages_per_user_by_type.groupby(\"userId\").pivot(\"page\").sum(\"page_visits\").na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1a5c1",
   "metadata": {},
   "source": [
    "## Get gender, level and is_mobile as 0/1. Get total time spent listening to music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4337f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = spark.sql('''\n",
    "    SELECT userId, \n",
    "        CASE WHEN first(gender) = 'F' THEN 1 ELSE 0 END AS gender,\n",
    "        CASE WHEN last(level) = 'paid' THEN 1 ELSE 0 END AS paid,\n",
    "        CASE WHEN first(userAgent) LIKE '%Mobi%' THEN 1 ELSE 0 END AS is_mobile,\n",
    "        SUM(nanvl(length, 0)) AS listening_time\n",
    "    FROM (SELECT * FROM events ORDER BY ts ASC)\n",
    "    GROUP BY userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aaf698",
   "metadata": {},
   "source": [
    "## Prepare final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c66f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pages_per_user_by_type.join(time_per_day, on=\"userId\") \\\n",
    "    .join(avg_session_len_per_user, on=\"userId\") \\\n",
    "    .join(user_data, on=\"userId\") \\\n",
    "    .join(avg_time_bw_sessions, on=\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b78ca0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 16:28:43 WARN TaskSetManager: Stage 156 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:44 WARN TaskSetManager: Stage 157 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:45 WARN TaskSetManager: Stage 158 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:46 WARN TaskSetManager: Stage 159 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:47 WARN TaskSetManager: Stage 160 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:49 WARN TaskSetManager: Stage 161 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:50 WARN TaskSetManager: Stage 162 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:28:51 WARN TaskSetManager: Stage 163 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-----------+-------------+------------------+---------------------+------+----+---------+--------------+--------------------+\n",
      "|userId|About|Add Friend|Add to Playlist|Cancel|Cancellation Confirmation|Downgrade|Error|Help|Home|Logout|NextSong|Roll Advert|Save Settings|Settings|Submit Downgrade|Submit Upgrade|Thumbs Down|Thumbs Up|Upgrade|first(days)|pages_per_day|avg_session_length|avg_items_per_session|gender|paid|is_mobile|listening_time|avg_time_bw_sessions|\n",
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-----------+-------------+------------------+---------------------+------+----+---------+--------------+--------------------+\n",
      "|     0|    0|         0|              0|     0|                        0|        0|    0|   0|   0|     0|       0|          0|            0|       0|               0|             0|          0|        0|      0|          0|            0|                 0|                    0|     0|   0|        0|           224|                  15|\n",
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-----------+-------------+------------------+---------------------+------+----+---------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in final.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109792f",
   "metadata": {},
   "source": [
    "We will handle missing values after the train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73aa4bf",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434fabf",
   "metadata": {},
   "source": [
    "Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "516681e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:01:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/09/13 11:01:40 WARN TaskSetManager: Stage 15 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:42 WARN TaskSetManager: Stage 16 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:43 WARN TaskSetManager: Stage 17 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:45 WARN TaskSetManager: Stage 18 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:47 WARN TaskSetManager: Stage 19 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:48 WARN TaskSetManager: Stage 20 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:50 WARN TaskSetManager: Stage 21 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:51 WARN TaskSetManager: Stage 22 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(train_df, test_df) = final.randomSplit([0.7, 0.3])\n",
    "\n",
    "# impute missing values in avg_time_bw_sessions\n",
    "imputer = Imputer(inputCol=\"avg_time_bw_sessions\", outputCol=\"avg_time_bw_sessions\")\n",
    "model = imputer.fit(train_df)\n",
    "train_df = model.transform(train_df)\n",
    "test_df = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e454f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301451b6",
   "metadata": {},
   "source": [
    "Including visits to the \"Cancel\" page would likely introduce data leakage. And we want to predict churn before the user cancels so let's drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b924e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(set(final.columns) - set([\"userId\", \"Cancel\", \"Cancellation Confirmation\", \"listening_time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ecce25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_98cedac9072e"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_assembler = VectorAssembler(outputCol=\"features\")\n",
    "vec_assembler.setInputCols(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5597fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = vec_assembler.transform(train_df).select(\"features\", final[\"Cancellation Confirmation\"].alias(\"label\"))\n",
    "testData = vec_assembler.transform(test_df).select(\"features\", final[\"Cancellation Confirmation\"].alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f0843",
   "metadata": {},
   "source": [
    "## Gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8ce1a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 16:56:33 WARN TaskSetManager: Stage 343 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:34 WARN TaskSetManager: Stage 344 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:35 WARN TaskSetManager: Stage 345 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:37 WARN TaskSetManager: Stage 346 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:38 WARN TaskSetManager: Stage 347 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:39 WARN TaskSetManager: Stage 348 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:41 WARN TaskSetManager: Stage 349 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:42 WARN TaskSetManager: Stage 350 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:46 WARN TaskSetManager: Stage 385 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:48 WARN TaskSetManager: Stage 386 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:49 WARN TaskSetManager: Stage 387 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:51 WARN TaskSetManager: Stage 388 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:53 WARN TaskSetManager: Stage 389 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:54 WARN TaskSetManager: Stage 390 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:56 WARN TaskSetManager: Stage 391 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:56:58 WARN TaskSetManager: Stage 392 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=5, labelCol=\"label\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "\n",
    "model = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99e50c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 16:57:52 WARN TaskSetManager: Stage 509 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:57:54 WARN TaskSetManager: Stage 510 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:57:55 WARN TaskSetManager: Stage 511 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:57:56 WARN TaskSetManager: Stage 512 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:57:57 WARN TaskSetManager: Stage 513 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:57:59 WARN TaskSetManager: Stage 514 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:58:00 WARN TaskSetManager: Stage 515 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:58:02 WARN TaskSetManager: Stage 516 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 16:58:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.transform(testData)\n",
    "predictions_df = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40e5b4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 0).sum()\n",
    "tp = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 2).sum()\n",
    "fn = (predictions_df[\"prediction\"] < predictions_df[\"label\"]).sum()\n",
    "fp = (predictions_df[\"prediction\"] > predictions_df[\"label\"]).sum()\n",
    "precision = tp / (tp+fp)\n",
    "recall = tp / (tp+fn)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70b595",
   "metadata": {},
   "source": [
    "Not the best results, let's try some cross validation and grid-searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be9560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34cffb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 17:01:22 WARN CacheManager: Asked to cache already cached data.) / 200]\n",
      "23/09/11 17:01:22 WARN CacheManager: Asked to cache already cached data.\n",
      "23/09/11 17:01:25 WARN TaskSetManager: Stage 785 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 17:01:26 WARN TaskSetManager: Stage 786 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 17:01:28 WARN TaskSetManager: Stage 787 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 17:01:29 WARN TaskSetManager: Stage 789 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 17:01:36 WARN TaskSetManager: Stage 791 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:11 WARN TaskSetManager: Stage 6334 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:13 WARN TaskSetManager: Stage 6335 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:15 WARN TaskSetManager: Stage 6336 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:20 WARN TaskSetManager: Stage 6339 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:23 WARN TaskSetManager: Stage 6344 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:26 WARN TaskSetManager: Stage 6345 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:28 WARN TaskSetManager: Stage 6347 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:06:35 WARN TaskSetManager: Stage 6349 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:17 WARN TaskSetManager: Stage 6532 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:19 WARN TaskSetManager: Stage 6533 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:21 WARN TaskSetManager: Stage 6534 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:26 WARN TaskSetManager: Stage 6543 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:27 WARN TaskSetManager: Stage 6537 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:31 WARN TaskSetManager: Stage 6541 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:32 WARN TaskSetManager: Stage 6542 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:07:34 WARN TaskSetManager: Stage 6544 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:16 WARN TaskSetManager: Stage 19317 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:18 WARN TaskSetManager: Stage 19318 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:19 WARN TaskSetManager: Stage 19319 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:25 WARN TaskSetManager: Stage 19326 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:28 WARN TaskSetManager: Stage 19327 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:29 WARN TaskSetManager: Stage 19329 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:36 WARN TaskSetManager: Stage 19331 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:41:37 WARN TaskSetManager: Stage 19333 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:13 WARN TaskSetManager: Stage 19515 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:14 WARN TaskSetManager: Stage 19516 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:16 WARN TaskSetManager: Stage 19517 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:20 WARN TaskSetManager: Stage 19525 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:22 WARN TaskSetManager: Stage 19520 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:25 WARN TaskSetManager: Stage 19524 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:33 WARN TaskSetManager: Stage 19529 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 18:42:35 WARN TaskSetManager: Stage 19530 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:51 WARN TaskSetManager: Stage 32300 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:52 WARN TaskSetManager: Stage 32301 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:54 WARN TaskSetManager: Stage 32302 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:55 WARN TaskSetManager: Stage 32303 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:57 WARN TaskSetManager: Stage 32304 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:13:58 WARN TaskSetManager: Stage 32305 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:00 WARN TaskSetManager: Stage 32306 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:02 WARN TaskSetManager: Stage 32307 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:06 WARN TaskSetManager: Stage 32342 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:10 WARN TaskSetManager: Stage 32343 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:13 WARN TaskSetManager: Stage 32344 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:17 WARN TaskSetManager: Stage 32345 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:20 WARN TaskSetManager: Stage 32346 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:24 WARN TaskSetManager: Stage 32347 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:28 WARN TaskSetManager: Stage 32348 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:32 WARN TaskSetManager: Stage 32349 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(maxIter=5, maxDepth=5, labelCol=\"label\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10, 50]) \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b1690a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0dca1888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1a690bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 19:14:43 WARN TaskSetManager: Stage 32694 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:45 WARN TaskSetManager: Stage 32695 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:46 WARN TaskSetManager: Stage 32696 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:48 WARN TaskSetManager: Stage 32697 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:49 WARN TaskSetManager: Stage 32698 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:50 WARN TaskSetManager: Stage 32699 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:52 WARN TaskSetManager: Stage 32700 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/11 19:14:53 WARN TaskSetManager: Stage 32701 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = cvModel.bestModel.transform(testData)\n",
    "predictions_df = predictions.toPandas()\n",
    "tn = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 0).sum()\n",
    "tp = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 2).sum()\n",
    "fn = (predictions_df[\"prediction\"] < predictions_df[\"label\"]).sum()\n",
    "fp = (predictions_df[\"prediction\"] > predictions_df[\"label\"]).sum()\n",
    "precision = tp / (tp+fp)\n",
    "recall = tp / (tp+fn)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0e3d9",
   "metadata": {},
   "source": [
    "Much better score when using a shallower tree. There will be a lot of variance on these models given the smaller dataset so we'll have to conduct a grid search on the full dataset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29a8cb",
   "metadata": {},
   "source": [
    "## LinearSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "523d783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa2f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b57d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:01:58 WARN TaskSetManager: Stage 63 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:01:59 WARN TaskSetManager: Stage 64 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:01 WARN TaskSetManager: Stage 65 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:02 WARN TaskSetManager: Stage 66 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:03 WARN TaskSetManager: Stage 67 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:05 WARN TaskSetManager: Stage 68 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:06 WARN TaskSetManager: Stage 69 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:07 WARN TaskSetManager: Stage 70 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r_scaler = RobustScaler(inputCol='features', outputCol=\"features_scaled\")\n",
    "robust_model = r_scaler.fit(trainingData)\n",
    "trainScaled = robust_model.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6588ee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', VectorUDT(), True), StructField('label', LongType(), True), StructField('features_scaled', VectorUDT(), True)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainScaled.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5eaf3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 11:02:11 WARN TaskSetManager: Stage 108 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:13 WARN TaskSetManager: Stage 109 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:14 WARN TaskSetManager: Stage 110 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:15 WARN TaskSetManager: Stage 111 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:17 WARN TaskSetManager: Stage 112 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:18 WARN TaskSetManager: Stage 113 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:20 WARN TaskSetManager: Stage 114 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:21 WARN TaskSetManager: Stage 115 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:25 WARN TaskSetManager: Stage 150 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:27 WARN TaskSetManager: Stage 151 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:28 WARN TaskSetManager: Stage 152 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:30 WARN TaskSetManager: Stage 153 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:31 WARN TaskSetManager: Stage 154 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:33 WARN TaskSetManager: Stage 155 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:34 WARN TaskSetManager: Stage 156 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:35 WARN TaskSetManager: Stage 157 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:40 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/09/13 11:02:48 WARN TaskSetManager: Stage 766 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:50 WARN TaskSetManager: Stage 767 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:52 WARN TaskSetManager: Stage 768 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:54 WARN TaskSetManager: Stage 769 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:56 WARN TaskSetManager: Stage 770 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:58 WARN TaskSetManager: Stage 771 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:02:59 WARN TaskSetManager: Stage 772 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:01 WARN TaskSetManager: Stage 773 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:04 WARN TaskSetManager: Stage 808 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:08 WARN TaskSetManager: Stage 809 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:11 WARN TaskSetManager: Stage 810 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:14 WARN TaskSetManager: Stage 811 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:17 WARN TaskSetManager: Stage 812 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:20 WARN TaskSetManager: Stage 813 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:24 WARN TaskSetManager: Stage 814 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/13 11:03:27 WARN TaskSetManager: Stage 815 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(regParam=0.1, featuresCol=\"features_scaled\")\n",
    "model = svm.fit(trainScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78cbffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testScaled = robust_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5540fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 15:56:28 WARN TaskSetManager: Stage 2896 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:31 WARN TaskSetManager: Stage 2897 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:32 WARN TaskSetManager: Stage 2898 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:34 WARN TaskSetManager: Stage 2899 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:36 WARN TaskSetManager: Stage 2900 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:37 WARN TaskSetManager: Stage 2901 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:39 WARN TaskSetManager: Stage 2902 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:56:40 WARN TaskSetManager: Stage 2903 contains a task of very large size (74761 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.46153846153846156)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.transform(testScaled)\n",
    "predictions_df = predictions.toPandas()\n",
    "tn = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 0).sum()\n",
    "tp = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 2).sum()\n",
    "fn = (predictions_df[\"prediction\"] < predictions_df[\"label\"]).sum()\n",
    "fp = (predictions_df[\"prediction\"] > predictions_df[\"label\"]).sum()\n",
    "precision = tp / (tp+fp)\n",
    "recall = tp / (tp+fn)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "579d93e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5314/1939349230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features_scaled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"areaUnderPR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, tol, rawPredictionCol, fitIntercept, standardization, threshold, weightCol, aggregationDepth, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \"\"\"\n\u001b[1;32m    737\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         self._java_obj = self._new_java_obj(\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;34m\"org.apache.spark.ml.classification.LinearSVC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1724\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: org does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(regParam=0.1, featuresCol=\"features_scaled\")\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
    "cv = CrossValidator(estimator=svm, estimatorParamMaps=grid, evaluator=evaluator, )\n",
    "\n",
    "cvModel = cv.fit(trainScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31430468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = cvModel.bestModel.transform(testScaled)\n",
    "predictions_df = predictions.toPandas()\n",
    "tn = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 0).sum()\n",
    "tp = (predictions_df[\"prediction\"] + predictions_df[\"label\"] == 2).sum()\n",
    "fn = (predictions_df[\"prediction\"] < predictions_df[\"label\"]).sum()\n",
    "fp = (predictions_df[\"prediction\"] > predictions_df[\"label\"]).sum()\n",
    "precision = tp / (tp+fp)\n",
    "recall = tp / (tp+fn)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70e708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

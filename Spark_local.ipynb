{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3982b234",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2079b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2d4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"XXXXXXX\"\n",
    "AWS_SECRET_KEY = \"XXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc455c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\n",
    "    f'org.apache.hadoop:hadoop-aws:3.3.1',\n",
    "    'com.google.guava:guava:30.1.1-jre',\n",
    "    'org.apache.httpcomponents:httpcore:4.4.14', \n",
    "    'com.google.inject:guice:4.2.2', \n",
    "    'com.google.inject.extensions:guice-servlet:4.2.2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c969ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 09:54:26 WARN Utils: Your hostname, DESKTOP-HT1RH4E resolves to a loopback address: 127.0.1.1; using 172.29.121.89 instead (on interface eth0)\n",
      "23/08/03 09:54:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/greg/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/greg/.ivy2/cache\n",
      "The jars for the packages stored in: /home/greg/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.httpcomponents#httpcore added as a dependency\n",
      "com.google.inject#guice added as a dependency\n",
      "com.google.inject.extensions#guice-servlet added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-00b0b72c-8046-4d99-a1b0-dcf2c6c7b380;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.guava#guava;30.1.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.8.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound com.google.inject#guice;4.2.2 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound com.google.inject.extensions#guice-servlet;4.2.2 in central\n",
      ":: resolution report :: resolve 351ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.inject#guice;4.2.2 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-servlet;4.2.2 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.8.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.guava#guava;25.1-android by [com.google.guava#guava;30.1.1-jre] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-00b0b72c-8046-4d99-a1b0-dcf2c6c7b380\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/9ms)\n",
      "23/08/03 09:54:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\\\n",
    "    .setMaster(\"local\")\\\n",
    "    .setAppName(\"pyspark-unittests\")\\\n",
    "    .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\\\n",
    "    .set('spark.jars.packages', ','.join(packages))\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf) \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2604011",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2474751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 09:54:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"s3a://udacity-dsnd/sparkify/mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c95231",
   "metadata": {},
   "source": [
    "# Process data for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1ca9",
   "metadata": {},
   "source": [
    "Filter out all not logged-in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30be67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f1d79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8a6bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_session_len_per_user = spark.sql('''\n",
    "    SELECT userId, AVG(length) AS avg_session_length FROM (\n",
    "        SELECT userId, MAX(ts) - MIN(ts) AS length FROM events\n",
    "        GROUP BY userId, sessionId\n",
    "    )\n",
    "    GROUP BY userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e49271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import last, udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc27e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------+\n",
      "|userId|sessionId|          end|\n",
      "+------+---------+-------------+\n",
      "|300011|       60|1538587993000|\n",
      "+------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session_ends = df.sort(\"ts\").groupby(df.userId, df.sessionId).agg(last(df.ts).alias(\"end\"))\n",
    "session_ends.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(FloatType())\n",
    "def time_bw_sessions(s: pd.Series) -> float:\n",
    "    return s.diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441b6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_bw_sessions = session_ends.sort(\"sessionId\").groupby(\"userId\").agg(time_bw_sessions(\"end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_day = spark.sql('''\n",
    "    SELECT events.userId, ROUND(COUNT(*) / first(a.days), 2) AS pages_per_day\n",
    "    FROM events\n",
    "    JOIN (\n",
    "        SELECT userId, GREATEST(1, ROUND((MAX(ts) - MIN(ts)) / 3600 / 24 / 1000, 2)) AS days FROM events\n",
    "        GROUP BY userId\n",
    "    ) AS a ON events.userId = a.userId\n",
    "    GROUP BY events.userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae403369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|userId|pages_per_day|\n",
      "+------+-------------+\n",
      "|100010|         8.62|\n",
      "|200002|        10.42|\n",
      "|   125|        550.0|\n",
      "|    51|       156.15|\n",
      "|   124|        80.42|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_per_day.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "543b5ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9ab19",
   "metadata": {},
   "source": [
    "Find how many times a user visits each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea26897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_per_user_by_type = spark.sql('''\n",
    "    SELECT userId, page, COUNT(*) AS page_visits\n",
    "    FROM events\n",
    "    GROUP BY userId, page\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec26b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, sum as fsum, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f395980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pages_per_user_by_type = pages_per_user_by_type.groupby(\"userId\").pivot(\"page\").sum(\"page_visits\").na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8c66f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pages_per_user_by_type.join(time_per_day, on=\"userId\").join(avg_session_len_per_user, on=\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b78ca0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-------------+------------------+\n",
      "|userId|About|Add Friend|Add to Playlist|Cancel|Cancellation Confirmation|Downgrade|Error|Help|Home|Logout|NextSong|Roll Advert|Save Settings|Settings|Submit Downgrade|Submit Upgrade|Thumbs Down|Thumbs Up|Upgrade|pages_per_day|avg_session_length|\n",
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-------------+------------------+\n",
      "|     0|    0|         0|              0|     0|                        0|        0|    0|   0|   0|     0|       0|          0|            0|       0|               0|             0|          0|        0|      0|            1|                 0|\n",
      "+------+-----+----------+---------------+------+-------------------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in final.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73aa4bf",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e454f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301451b6",
   "metadata": {},
   "source": [
    "Including visits to the \"Cancel\" page would likely introduce data leakage. And we want to predict churn before the user cancels so let's drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60b924e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(set(final.columns) - set([\"userId\", \"Cancel\", \"Cancellation Confirmation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b238419d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Logout',\n",
       " 'Thumbs Up',\n",
       " 'pages_per_day',\n",
       " 'Help',\n",
       " 'NextSong',\n",
       " 'Save Settings',\n",
       " 'Add Friend',\n",
       " 'Roll Advert',\n",
       " 'Upgrade',\n",
       " 'Thumbs Down',\n",
       " 'Submit Upgrade',\n",
       " 'Add to Playlist',\n",
       " 'Settings',\n",
       " 'avg_session_length',\n",
       " 'Home',\n",
       " 'Downgrade',\n",
       " 'Submit Downgrade',\n",
       " 'Error',\n",
       " 'About']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83ecce25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_9854c319a9b4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_assembler = VectorAssembler(outputCol=\"features\")\n",
    "vec_assembler.setInputCols(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "732521fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7286c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vec_assembler.transform(final).select(\"features\", final[\"Cancellation Confirmation\"].alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "516681e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=5, labelCol=\"label\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "\n",
    "model = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99e50c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4976fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "826dcd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "confusion_mat = predictions.select(\n",
    "    fsum(when(predictions.label + predictions.prediction == 0, 1).otherwise(0)).alias(\"TN\"),\n",
    "    fsum(when(predictions.label + predictions.prediction == 2, 1).otherwise(0)).alias(\"TP\"),\n",
    "    fsum(when(predictions.label > predictions.prediction, 1).otherwise(0)).alias(\"FN\"),\n",
    "    fsum(when(predictions.label < predictions.prediction, 1).otherwise(0)).alias(\"FP\")\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "62f8e30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6363636363636364, 0.5)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = confusion_mat[0][\"TP\"] / (confusion_mat[0][\"TP\"] + confusion_mat[0][\"FP\"])\n",
    "recall = confusion_mat[0][\"TP\"] / (confusion_mat[0][\"TP\"] + confusion_mat[0][\"FN\"])\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70b595",
   "metadata": {},
   "source": [
    "Not the best results, let's try some cross validation and grid-searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34cffb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import tempfile\n",
    "\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=5, labelCol=\"label\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10, 50]) \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "cvModel = cv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b1690a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0dca1888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.getMaxIter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a9601",
   "metadata": {},
   "source": [
    "Much better scores after CV, but this is not a true model scoring as the model has seen the test set during training. There is not enough data to split further into an eval set, but we can use these params as a base for further tuning on the whole dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e1a690bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.7857142857142857)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = cvModel.bestModel.transform(testData)\n",
    "confusion_mat = predictions.select(\n",
    "    fsum(when(predictions.label + predictions.prediction == 0, 1).otherwise(0)).alias(\"TN\"),\n",
    "    fsum(when(predictions.label + predictions.prediction == 2, 1).otherwise(0)).alias(\"TP\"),\n",
    "    fsum(when(predictions.label > predictions.prediction, 1).otherwise(0)).alias(\"FN\"),\n",
    "    fsum(when(predictions.label < predictions.prediction, 1).otherwise(0)).alias(\"FP\")\n",
    ").collect()\n",
    "precision = confusion_mat[0][\"TP\"] / (confusion_mat[0][\"TP\"] + confusion_mat[0][\"FP\"])\n",
    "recall = confusion_mat[0][\"TP\"] / (confusion_mat[0][\"TP\"] + confusion_mat[0][\"FN\"])\n",
    "precision, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
